{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Clustering with Python\n",
    "\n",
    "The dataset is a list of the top 100 films of all time from an IMDB user list called Top 100 Greatest Movies of All Time (The Ultimate List) by [ChrisWalczyk55](https://www.imdb.com/list/ls055592025/). It had been originally scraped by Brandon Rose (see https://github.com/brandomr/document_cluster).\n",
    "\n",
    "We will exploring different clustering methods to find out which movies (based on their synopsis) are grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lists: titles, imdb and wikipedia synopses\n",
    "titles = open('../../DataDirectory/100movies/title_list.txt').read().split('\\n')\n",
    "#ensures that only the first 100 are read in\n",
    "titles = titles[:100]\n",
    "\n",
    "synopses_wiki = open('../../DataDirectory/100movies/synopses_list_wiki.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_wiki = synopses_wiki[:100]\n",
    "\n",
    "synopses_clean_wiki = []\n",
    "for text in synopses_wiki:\n",
    "    #strips html formatting and converts to unicode\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    synopses_clean_wiki.append(text)\n",
    "\n",
    "synopses_wiki = synopses_clean_wiki\n",
    "\n",
    "synopses_imdb = open('../../DataDirectory/100movies/synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_imdb = synopses_imdb[:100]\n",
    "\n",
    "synopses_clean_imdb = []\n",
    "for text in synopses_imdb:\n",
    "    #strips html formatting and converts to unicode\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    synopses_clean_imdb.append(text)\n",
    "\n",
    "synopses_imdb = synopses_clean_imdb\n",
    "\n",
    "# combine the synopses \n",
    "synopses = []\n",
    "for i in range(len(synopses_wiki)):\n",
    "    item = synopses_wiki[i] + synopses_imdb[i]\n",
    "    synopses.append(item)\n",
    "\n",
    "#checks\n",
    "print(str(len(titles)) + ' titles')\n",
    "print(str(len(synopses_wiki)) + ' wiki synopses')\n",
    "print(str(len(synopses_imdb)) + ' imdb synopses')\n",
    "print(str(len(synopses)) + ' combined synopses')\n",
    "\n",
    "#put everything into one data frame\n",
    "data_array = np.array([titles, synopses])\n",
    "data = pd.DataFrame(data=data_array).T\n",
    "data.columns =['title', 'contents']\n",
    "#we will use this for some inspections later\n",
    "data['rank'] = data.index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the term frequency features\n",
    "\n",
    "RECAP: TF-IDF stands for \"Term Frequency â€” Inverse Document Frequency\"\n",
    "\n",
    "The rationale behind TF-IDF is the following:\n",
    "\n",
    "* a word that frequently appears in a document has more relevancy for that document, meaning that there is higher probability that the document is about or in relation to that specific word\n",
    "* a word that frequently appears in more documents may prevent us from finding the right document in a collection; the word is relevant either for all documents or for none. Either way, it will not help us filter out a single document or a small subset of documents from the whole set.\n",
    "\n",
    "So then TF-IDF is a score which is applied to every word in every document in our dataset. And for every word, the TF-IDF value increases with every appearance of the word in a document, but is gradually decreased with every appearance in other documents. And the maths for that is in the next section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple things to note about the parameters used in the Sklearn TfidfVectorizer below:\n",
    "\n",
    "* max_df: this is the maximum frequency within the documents a given feature can have to be used in the tfi-idf matrix. If the term is in greater than 80% of the documents it probably cares little meanining (in the context of film synopses)\n",
    "* min_idf: this could be an integer (e.g. 5) and the term would have to be in at least 5 of the documents to be considered. Here we pass 0.2; the term must be in at least 20% of the document. It can be found that if we allowed a lower min_df we end up basing clustering on names--for example \"Michael\" or \"Tom\" are names found in several of the movies and the synopses use these names frequently, but the names carry no real meaning.\n",
    "* ngram_range: this just means we'll look at unigrams, bigrams and trigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under the hood, TfidfVectorizer computes the word counts, IDF values, and Tf-idf scores all using the same dataset.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df = 0.2,\n",
    "    max_df = 0.8,\n",
    "    max_features = 200000,\n",
    "    stop_words = 'english',\n",
    "    ngram_range=(1,3)\n",
    ")\n",
    "\n",
    "text = tfidf.fit_transform(data.contents)\n",
    "terms = tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "#load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_stemmed = TfidfVectorizer(\n",
    "    min_df=0.2,\n",
    "    max_df=0.8, \n",
    "    max_features=200000,\n",
    "    stop_words='english',\n",
    "    tokenizer=tokenize_and_stem, \n",
    "    ngram_range=(1,3)\n",
    ")\n",
    "\n",
    "text = tfidf_stemmed.fit_transform(data.contents)\n",
    "terms = tfidf_stemmed.get_feature_names_out()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Means clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Elbow method to find out number of clusters (SSE plot - Sum of Squared errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch kmeans method - MiniBatchKMeans is faster, but gives slightly different results\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def find_optimal_clusters(data, max_k):\n",
    "    iters = range(1, max_k+1, 1)\n",
    "    \n",
    "    sse = []\n",
    "    for k in iters:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n",
    "        #sse.append(KMeans(n_clusters=k, n_init=10, random_state=20).fit(data).inertia_)\n",
    "        print('Fit {} clusters'.format(k))\n",
    "        \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(iters, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centers')\n",
    "    ax.set_xticks(iters)\n",
    "    ax.set_xticklabels(iters)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE by Cluster Center Plot')\n",
    "    \n",
    "find_optimal_clusters(text, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km_num_clusters = 5\n",
    "\n",
    "#n_init Number of times the k-means algorithm is run with different centroid seeds\n",
    "km = KMeans(n_clusters=km_num_clusters, n_init = 10, random_state=20)\n",
    "km_clusters = km.fit_predict(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Means Results\n",
    "\n",
    "Some preparations to investigate the clustering results.\n",
    "\n",
    "a) How many films per cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['km_cluster'] = km_clusters\n",
    "data['km_cluster'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) What is the average ranking per cluster? Original ranking (Top 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data['rank'].groupby(data['km_cluster'])\n",
    "grouped.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) List of terms and titles in each cluster? To investigate what was clustered together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(features, clusters, labels, n_terms):\n",
    "    df = pd.DataFrame(features.todense()).groupby(clusters).mean()\n",
    "    \n",
    "    for i,r in df.iterrows():\n",
    "        print(\"\\nCluster {} terms: \".format(i), end='')\n",
    "        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]), end='')\n",
    "    \n",
    "get_top_keywords(text, km_clusters, terms, 10)\n",
    "print();\n",
    "print();\n",
    "\n",
    "km_titles = data.groupby('km_cluster').head(5).reset_index(drop=True)\n",
    "for i in range(km_num_clusters):\n",
    "    print(\"Cluster %d titles: \" % i, end='')\n",
    "    print(km_titles[km_titles['km_cluster'] == i]['title'].to_list())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering\n",
    "\n",
    "#### With scipy: Cosine similarity and Ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster hierarchy package from scipy\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "#need distance matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(text)\n",
    "\n",
    "#define the linkage_matrix using ward clustering pre-computed distances\n",
    "ward_linkage_matrix = ward(dist) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(ward_linkage_matrix, orientation=\"right\", labels=titles);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "#uncomment below to save figure\n",
    "plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fcluster method is used to predict labels on the data\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "ward_num_clusters = 5\n",
    "\n",
    "ward_clusters = fcluster(\n",
    "    ward_linkage_matrix, \n",
    "    ward_num_clusters, \n",
    "    criterion='maxclust'\n",
    ")\n",
    "\n",
    "# add WARD results\n",
    "data['ward_cluster'] = ward_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_keywords(text, ward_clusters, terms, 10)\n",
    "print();\n",
    "print();\n",
    "\n",
    "ward_titles = data.groupby('ward_cluster').head(5).reset_index(drop=True)\n",
    "for i in range(1, ward_num_clusters+1):\n",
    "    print(\"Cluster %d titles: \" % i, end='')\n",
    "    print(ward_titles[ward_titles['ward_cluster'] == i]['title'].to_list())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scipy Cosine similarity and average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import average\n",
    "#define the linkage_matrix using average clustering pre-computed distances\n",
    "average_linkage_matrix = average(dist) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(average_linkage_matrix, orientation=\"right\", labels=titles);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "#uncomment below to save figure\n",
    "plt.savefig('average_clusters.png', dpi=200) #save figure as ward_clusters\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "sklearn_num_clusters = 5\n",
    "\n",
    "cluster_model = AgglomerativeClustering(\n",
    "    n_clusters = sklearn_num_clusters, \n",
    "    metric = 'precomputed', \n",
    "    linkage = 'average')\n",
    "\n",
    "sklearn_cluster = cluster_model.fit_predict(dist) #using cosine similarity distance matrix\n",
    "\n",
    "# add sklearn results\n",
    "data['sklearn_cluster'] = sklearn_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_keywords(text, sklearn_cluster, terms, 10)\n",
    "print();\n",
    "print();\n",
    "\n",
    "sklearn_titles = data.groupby('sklearn_cluster').head(5).reset_index(drop=True)\n",
    "for i in range(sklearn_num_clusters):\n",
    "    print(\"Cluster %d titles: \" % i, end='')\n",
    "    print(sklearn_titles[sklearn_titles['sklearn_cluster'] == i]['title'].to_list())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spring2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
